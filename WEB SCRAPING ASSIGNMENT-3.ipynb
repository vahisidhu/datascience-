{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a90d1ba",
   "metadata": {},
   "source": [
    "# WEB SCRAPING-ASSIGNMENT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27bc1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a python program which searches all the product under a particular product from www.amazon.in. The\n",
    "product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for\n",
    "guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "902cbbfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boto3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Replace these with your own access and secret keys\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'boto3'"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "# Replace these with your own access and secret keys\n",
    "ACCESS_KEY = 'your_access_key'\n",
    "SECRET_KEY = 'your_secret_key'\n",
    "ASSOCIATE_TAG = 'your_associate_tag'\n",
    "\n",
    "def search_amazon_products(query):\n",
    "    config = Config(\n",
    "        region_name='us-east-1',\n",
    "        signature_version='v4',\n",
    "        retries={\n",
    "            'max_attempts': 10,\n",
    "            'mode': 'standard'\n",
    "        }\n",
    "    )\n",
    "    client = boto3.client(\n",
    "        'advertising',\n",
    "        aws_access_key_id=ACCESS_KEY,\n",
    "        aws_secret_access_key=SECRET_KEY,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    response = client.search_items(\n",
    "        Keywords=query,\n",
    "        ItemPage=1,\n",
    "        Resources=[\n",
    "            'ItemInfo.Title', 'Offers.Listings.Price'\n",
    "        ],\n",
    "        PartnerTag=ASSOCIATE_TAG,\n",
    "        PartnerType='Associates',\n",
    "        Marketplace='www.amazon.in'\n",
    "    )\n",
    "    \n",
    "    items = response.get('Items', [])\n",
    "    for item in items:\n",
    "        title = item['ItemInfo']['Title']['DisplayValue']\n",
    "        price = item['Offers']['Listings'][0]['Price']['Dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75810a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. In the above question, now scrape the following details of each product listed in first 3 pages of your search\n",
    "results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then\n",
    "scrape all the products available under that product name. Details to be scraped are: \"Brand\n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and\n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b07b7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product to search for: SOUP\n",
      "Failed to retrieve page 1\n",
      "Saved 0 products to amazon_products.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_product_details(soup):\n",
    "    product_list = []\n",
    "    products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "    \n",
    "    for product in products:\n",
    "        brand_name = product.find('span', {'class': 'a-size-base-plus'}).text.strip() if product.find('span', {'class': 'a-size-base-plus'}) else '-'\n",
    "        product_name = product.find('span', {'class': 'a-size-medium a-color-base a-text-normal'}).text.strip() if product.find('span', {'class': 'a-size-medium a-color-base a-text-normal'}) else '-'\n",
    "        price = product.find('span', {'class': 'a-offscreen'}).text.strip() if product.find('span', {'class': 'a-offscreen'}) else '-'\n",
    "        return_exchange = '-'\n",
    "        expected_delivery = '-'\n",
    "        availability = '-'\n",
    "        product_url = 'https://www.amazon.in' + product.find('a', {'class': 'a-link-normal a-text-normal'})['href'] if product.find('a', {'class': 'a-link-normal a-text-normal'}) else '-'\n",
    "        \n",
    "        product_list.append({\n",
    "            'Brand Name': brand_name,\n",
    "            'Name of the Product': product_name,\n",
    "            'Price': price,\n",
    "            'Return/Exchange': return_exchange,\n",
    "            'Expected Delivery': expected_delivery,\n",
    "            'Availability': availability,\n",
    "            'Product URL': product_url\n",
    "        })\n",
    "    \n",
    "    return product_list\n",
    "\n",
    "def scrape_amazon(query, pages=3):\n",
    "    base_url = \"https://www.amazon.in/s\"\n",
    "    all_products = []\n",
    "    \n",
    "    for page in range(1, pages + 1):\n",
    "        params = {\n",
    "            'k': query,\n",
    "            'page': page\n",
    "        }\n",
    "        \n",
    "        response = requests.get(base_url, headers=headers, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve page {page}\")\n",
    "            break\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        products = get_product_details(soup)\n",
    "        all_products.extend(products)\n",
    "        \n",
    "        # Sleep to avoid being blocked by Amazon\n",
    "        time.sleep(2)\n",
    "    \n",
    "    return all_products\n",
    "\n",
    "def save_to_csv(data, filename='amazon_products.csv'):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Enter the product to search for: \")\n",
    "    products = scrape_amazon(query)\n",
    "    save_to_csv(products)\n",
    "    print(f\"Saved {len(products)} products to amazon_products.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a42dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    ". Write a python program to access the search bar and search button on images.google.com and scrape 10\n",
    "images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e9c7f41",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m By\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeys\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Keys\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "# Function to download images\n",
    "def download_image(url, folder_path, image_name):\n",
    "    try:\n",
    "        img_data = requests.get(url).content\n",
    "        with open(os.path.join(folder_path, image_name), 'wb') as handler:\n",
    "            handler.write(img_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not download {url} - {e}\")\n",
    "\n",
    "# Function to perform the image scraping\n",
    "def scrape_images(keyword, num_images=10):\n",
    "    # Set up the browser\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Run in headless mode\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    # Go to Google Images\n",
    "    driver.get('https://images.google.com/')\n",
    "    search_box = driver.find_element(By.NAME, 'q')\n",
    "    search_box.send_keys(keyword)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Scroll and collect image URLs\n",
    "    image_urls = set()\n",
    "    while len(image_urls) < num_images:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Give time for images to load\n",
    "        thumbnails = driver.find_elements(By.CSS_SELECTOR, 'img.rg_i')\n",
    "        for img in thumbnails:\n",
    "            try:\n",
    "                img.click()\n",
    "                time.sleep(2)\n",
    "                images = driver.find_elements(By.CSS_SELECTOR, 'img.n3VNCb')\n",
    "                for image in images:\n",
    "                    src = image.get_attribute('src')\n",
    "                    if src and 'http' in src:\n",
    "                        image_urls.add(src)\n",
    "                        if len(image_urls) >= num_images:\n",
    "                            break\n",
    "            except Exception as e:\n",
    "                print(f\"Error clicking image: {e}\")\n",
    "            if len(image_urls) >= num_images:\n",
    "                break\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Download the images\n",
    "    folder_path = f\"images/{keyword.replace(' ', '_')}\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    for i, url in enumerate(image_urls):\n",
    "        download_image(url, folder_path, f\"{keyword.replace(' ', '_')}_{i + 1}.jpg\")\n",
    "\n",
    "    print(f\"Downloaded {len(image_urls)} images for keyword: {keyword}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "    for keyword in keywords:\n",
    "        scrape_images(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b07d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com\n",
    "and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand\n",
    "Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,\n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the\n",
    "details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55bc5e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the smartphone to search for: BeautifulSoup\n",
      "Failed to retrieve page: 500\n",
      "Saved 0 smartphones to flipkart_smartphones.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_smartphone_details(soup):\n",
    "    smartphone_list = []\n",
    "    products = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "\n",
    "    for product in products:\n",
    "        try:\n",
    "            brand_name = product.find('div', {'class': '_4rR01T'}).text.split(' ')[0] if product.find('div', {'class': '_4rR01T'}) else '-'\n",
    "            smartphone_name = product.find('div', {'class': '_4rR01T'}).text if product.find('div', {'class': '_4rR01T'}) else '-'\n",
    "            price = product.find('div', {'class': '_30jeq3 _1_WHN1'}).text if product.find('div', {'class': '_30jeq3 _1_WHN1'}) else '-'\n",
    "            product_url = 'https://www.flipkart.com' + product.find('a', {'class': '_1fQZEK'})['href'] if product.find('a', {'class': '_1fQZEK'}) else '-'\n",
    "\n",
    "            details = product.find('ul', {'class': '_1xgFaf'}).find_all('li') if product.find('ul', {'class': '_1xgFaf'}) else []\n",
    "            color, ram, storage, primary_camera, secondary_camera, display_size, battery_capacity = '-', '-', '-', '-', '-', '-', '-'\n",
    "            for detail in details:\n",
    "                text = detail.text\n",
    "                if 'Color' in text:\n",
    "                    color = text.split(':')[1].strip()\n",
    "                elif 'RAM' in text:\n",
    "                    ram = text.split('|')[0].strip()\n",
    "                    storage = text.split('|')[1].strip()\n",
    "                elif 'Primary Camera' in text:\n",
    "                    primary_camera = text.split('|')[0].strip()\n",
    "                    secondary_camera = text.split('|')[1].strip()\n",
    "                elif 'Display' in text:\n",
    "                    display_size = text.split(':')[1].strip()\n",
    "                elif 'Battery' in text:\n",
    "                    battery_capacity = text.split(':')[1].strip()\n",
    "\n",
    "            smartphone_list.append({\n",
    "                'Brand Name': brand_name,\n",
    "                'Smartphone Name': smartphone_name,\n",
    "                'Colour': color,\n",
    "                'RAM': ram,\n",
    "                'Storage(ROM)': storage,\n",
    "                'Primary Camera': primary_camera,\n",
    "                'Secondary Camera': secondary_camera,\n",
    "                'Display Size': display_size,\n",
    "                'Battery Capacity': battery_capacity,\n",
    "                'Price': price,\n",
    "                'Product URL': product_url\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing product: {e}\")\n",
    "\n",
    "    return smartphone_list\n",
    "\n",
    "def scrape_flipkart_smartphones(query):\n",
    "    base_url = \"https://www.flipkart.com/search\"\n",
    "    params = {'q': query}\n",
    "    response = requests.get(base_url, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    smartphones = get_smartphone_details(soup)\n",
    "    return smartphones\n",
    "\n",
    "def save_to_csv(data, filename='flipkart_smartphones.csv'):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Enter the smartphone to search for: \")\n",
    "    smartphones = scrape_flipkart_smartphones(query)\n",
    "    save_to_csv(smartphones)\n",
    "    print(f\"Saved {len(smartphones)} smartphones to flipkart_smartphones.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdccfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "125221a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m By\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeys\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Keys\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "def get_coordinates(city):\n",
    "    # Set up the browser\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Run in headless mode\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    # Go to Google Maps\n",
    "    driver.get('https://maps.google.com')\n",
    "\n",
    "    # Find the search box and enter the city name\n",
    "    search_box = driver.find_element(By.ID, 'searchboxinput')\n",
    "    search_box.send_keys(city)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait for the results to load\n",
    "    time.sleep(5)  # Adjust the sleep time if needed\n",
    "\n",
    "    # Extract the coordinates from the URL\n",
    "    current_url = driver.current_url\n",
    "    driver.quit()\n",
    "\n",
    "    try:\n",
    "        # Extract latitude and longitude from the URL\n",
    "        if \"/@-?[0-9]+\" in current_url:\n",
    "            coordinates = current_url.split('/@')[1].split(',')[0:2]\n",
    "            latitude = coordinates[0]\n",
    "            longitude = coordinates[1]\n",
    "            return float(latitude), float(longitude)\n",
    "        else:\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting coordinates: {e}\")\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city = input(\"Enter the city name to search for: \")\n",
    "    latitude, longitude = get_coordinates(city)\n",
    "    if latitude and longitude:\n",
    "        print(f\"Coordinates of {city}:\")\n",
    "        print(f\"Latitude: {latitude}\")\n",
    "        print(f\"Longitude: {longitude}\")\n",
    "    else:\n",
    "        print(f\"Could not find coordinates for {city}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96c5415",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a program to scrap all the available details of best gaming laptops from digit.in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c8180d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 0 laptops to digit_gaming_laptops.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_laptop_details(soup):\n",
    "    laptop_list = []\n",
    "\n",
    "    laptops = soup.find_all('div', class_='specs-box')\n",
    "\n",
    "    for laptop in laptops:\n",
    "        try:\n",
    "            name = laptop.find('h3').text.strip()\n",
    "            details = laptop.find_all('div', class_='value')\n",
    "            specs = {\n",
    "                'Laptop Name': name,\n",
    "                'Processor': details[0].text.strip() if len(details) > 0 else '-',\n",
    "                'Graphics': details[1].text.strip() if len(details) > 1 else '-',\n",
    "                'RAM': details[2].text.strip() if len(details) > 2 else '-',\n",
    "                'Storage': details[3].text.strip() if len(details) > 3 else '-',\n",
    "                'Display': details[4].text.strip() if len(details) > 4 else '-',\n",
    "                'Price': details[5].text.strip() if len(details) > 5 else '-'\n",
    "            }\n",
    "            laptop_list.append(specs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing laptop: {e}\")\n",
    "\n",
    "    return laptop_list\n",
    "\n",
    "def scrape_digit_gaming_laptops():\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    laptops = get_laptop_details(soup)\n",
    "    return laptops\n",
    "\n",
    "def save_to_csv(data, filename='digit_gaming_laptops.csv'):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    laptops = scrape_digit_gaming_laptops()\n",
    "    save_to_csv(laptops)\n",
    "    print(f\"Saved {len(laptops)} laptops to digit_gaming_laptops.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac5c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped:\n",
    "“Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8905b797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 0 billionaires to forbes_billionaires.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_billionaire_details(soup):\n",
    "    billionaire_list = []\n",
    "    \n",
    "    rows = soup.find_all('div', class_='personName')\n",
    "    \n",
    "    for row in rows:\n",
    "        try:\n",
    "            rank = row.find('div', class_='rank').text.strip() if row.find('div', class_='rank') else '-'\n",
    "            name = row.find('div', class_='personName').text.strip() if row.find('div', class_='personName') else '-'\n",
    "            net_worth = row.find('div', class_='netWorth').text.strip() if row.find('div', class_='netWorth') else '-'\n",
    "            age = row.find('div', class_='age').text.strip() if row.find('div', class_='age') else '-'\n",
    "            citizenship = row.find('div', class_='countryOfCitizenship').text.strip() if row.find('div', class_='countryOfCitizenship') else '-'\n",
    "            source = row.find('div', class_='source').text.strip() if row.find('div', class_='source') else '-'\n",
    "            industry = row.find('div', class_='category').text.strip() if row.find('div', class_='category') else '-'\n",
    "            \n",
    "            billionaire_list.append({\n",
    "                'Rank': rank,\n",
    "                'Name': name,\n",
    "                'Net worth': net_worth,\n",
    "                'Age': age,\n",
    "                'Citizenship': citizenship,\n",
    "                'Source': source,\n",
    "                'Industry': industry\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row: {e}\")\n",
    "\n",
    "    return billionaire_list\n",
    "\n",
    "def scrape_forbes_billionaires():\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    billionaires = get_billionaire_details(soup)\n",
    "    return billionaires\n",
    "\n",
    "def save_to_csv(data, filename='forbes_billionaires.csv'):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    billionaires = scrape_forbes_billionaires()\n",
    "    save_to_csv(billionaires)\n",
    "    print(f\"Saved {len(billionaires)} billionaires to forbes_billionaires.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367120b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted\n",
    "from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f651f91",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'googleapiclient'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogleapiclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscovery\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define your API key\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'googleapiclient'"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "\n",
    "# Define your API key\n",
    "API_KEY = 'YOUR_API_KEY'\n",
    "\n",
    "# YouTube video ID\n",
    "VIDEO_ID = 'YOUR_VIDEO_ID'\n",
    "\n",
    "# Create a build object\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "def get_video_comments(video_id, max_results):\n",
    "    comments = []\n",
    "    page_token = None\n",
    "\n",
    "    while len(comments) < max_results:\n",
    "        # Get the video comments\n",
    "        request = youtube.commentThreads().list(\n",
    "            part='snippet',\n",
    "            videoId=video_id,\n",
    "            maxResults=100,\n",
    "            pageToken=page_token,\n",
    "            textFormat='plainText'\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        # Parse the response\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comments.append({\n",
    "                'Comment': comment['textDisplay'],\n",
    "                'Upvotes': comment['likeCount'],\n",
    "                'Time': comment['publishedAt']\n",
    "            })\n",
    "\n",
    "        # Check if there is a next page\n",
    "        if 'nextPageToken' in response and len(comments) < max_results:\n",
    "            page_token = response['nextPageToken']\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return comments[:max_results]\n",
    "\n",
    "def save_to_csv(data, filename='youtube_comments.csv'):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    max_results = 500\n",
    "    comments = get_video_comments(VIDEO_ID, max_results)\n",
    "    save_to_csv(comments)\n",
    "    print(f\"Saved {len(comments)} comments to youtube_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154140be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in\n",
    "“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall\n",
    "reviews, privates from price, dorms from price, facilities and property description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f0307d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://www.hostelworld.com/hostels/London",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Request the webpage\u001b[39;00m\n\u001b[0;32m      9\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m---> 10\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Parse the HTML content\u001b[39;00m\n\u001b[0;32m     13\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m     )\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://www.hostelworld.com/hostels/London"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL for hostels in London\n",
    "url = \"https://www.hostelworld.com/hostels/London\"\n",
    "\n",
    "# Request the webpage\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Function to extract hostel details\n",
    "def get_hostel_details(hostel):\n",
    "    name = hostel.find('h2', class_='title').text.strip() if hostel.find('h2', class_='title') else '-'\n",
    "    distance = hostel.find('span', class_='distance-description').text.strip() if hostel.find('span', class_='distance-description') else '-'\n",
    "    rating = hostel.find('div', class_='score orange big').text.strip() if hostel.find('div', class_='score orange big') else '-'\n",
    "    total_reviews = hostel.find('div', class_='reviews').text.strip() if hostel.find('div', class_='reviews') else '-'\n",
    "    overall_reviews = hostel.find('div', class_='keyword').text.strip() if hostel.find('div', class_='keyword') else '-'\n",
    "    privates_from_price = hostel.find('div', class_='price privates').text.strip() if hostel.find('div', class_='price privates') else '-'\n",
    "    dorms_from_price = hostel.find('div', class_='price dorms').text.strip() if hostel.find('div', class_='price dorms') else '-'\n",
    "    facilities = ', '.join([fac.text.strip() for fac in hostel.find_all('div', class_='facilities-label facility')])\n",
    "    description = hostel.find('div', class_='rating-fab-text').text.strip() if hostel.find('div', class_='rating-fab-text') else '-'\n",
    "    \n",
    "    return {\n",
    "        'Hostel Name': name,\n",
    "        'Distance from City Centre': distance,\n",
    "        'Ratings': rating,\n",
    "        'Total Reviews': total_reviews,\n",
    "        'Overall Reviews': overall_reviews,\n",
    "        'Privates From Price': privates_from_price,\n",
    "        'Dorms From Price': dorms_from_price,\n",
    "        'Facilities': facilities,\n",
    "        'Property Description': description\n",
    "    }\n",
    "\n",
    "# Extract hostel information\n",
    "hostel_list = []\n",
    "hostels = soup.find_all('div', class_='fabresult')\n",
    "for hostel in hostels:\n",
    "    details = get_hostel_details(hostel)\n",
    "    hostel_list.append(details)\n",
    "\n",
    "# Save the data to a CSV file\n",
    "def save_to_csv(data, filename='hostelworld_london_hostels.csv'):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_to_csv(hostel_list)\n",
    "    print(f\"Saved {len(hostel_list)} hostels to hostelworld_london_hostels.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a4fda6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
